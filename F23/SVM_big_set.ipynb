{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       username                                        description hand.label  \\\n",
      "0   Casper30214  Army Civil Service(Retired);Military Ops Resea...      other   \n",
      "1         enckj  Former EPA Regional Administrator, President o...       acad   \n",
      "2  nuclearkelly  Scientist at ORNL, DOE Early Career Awardee, F...       acad   \n",
      "3       stukhan  Dad. Director of the Australian Graduate Schoo...       acad   \n",
      "4       PatMag7  Podcasting about Feminist Participatory Action...       acad   \n",
      "\n",
      "                              description_lemmatized  \n",
      "0  ['army', 'civil', 'service', '(', 'retired', '...  \n",
      "1  ['former', 'epa', 'regional', 'administrator',...  \n",
      "2  ['scientist', 'at', 'ornl', ',', 'doe', 'early...  \n",
      "3  ['dad', '.', 'director', 'of', 'the', 'austral...  \n",
      "4  ['podcasting', 'about', 'feminist', 'participa...  \n",
      "\n",
      "       username                                        description hand.label  \\\n",
      "0   Casper30214  Army Civil Service(Retired);Military Ops Resea...      other   \n",
      "1         enckj  Former EPA Regional Administrator, President o...       acad   \n",
      "2  nuclearkelly  Scientist at ORNL, DOE Early Career Awardee, F...       acad   \n",
      "3       stukhan  Dad. Director of the Australian Graduate Schoo...       acad   \n",
      "4       PatMag7  Podcasting about Feminist Participatory Action...       acad   \n",
      "\n",
      "                              description_lemmatized  \n",
      "0  ['army', 'civil', 'service', '(', 'retired', '...  \n",
      "1  ['former', 'epa', 'regional', 'administrator',...  \n",
      "2  ['scientist', 'at', 'ornl', ',', 'doe', 'early...  \n",
      "3  ['dad', '.', 'director', 'of', 'the', 'austral...  \n",
      "4  ['podcasting', 'about', 'feminist', 'participa...  \n",
      "\n",
      "Number of labels per category:\n",
      "hand.label\n",
      "other    7232\n",
      "media     397\n",
      "acad      187\n",
      "gov        36\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the Data: from naivebayes2 (Melvin Adkins work)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "filepath = \"finalized_8K_accounts.csv\"\n",
    "hand_label = \"hand.label\"\n",
    "government = \"gov\"\n",
    "academia = \"acad\"\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "df = df[((df[hand_label]=='media') | (df[hand_label]== academia) | (df[hand_label]==government) | (df[hand_label]=='other' ))]\n",
    "\n",
    "\n",
    "df = df[['username','description',hand_label]] # keep only relevant columns\n",
    "\n",
    "\n",
    "# Preprocessing step - lemmatization on description column\n",
    "words_not_changed = ['media']\n",
    "\n",
    "def preprocessing(row):\n",
    "\n",
    "    before = []\n",
    "    after = []\n",
    "    if str(row) == \"nan\":\n",
    "        row = \"\"\n",
    "    else:\n",
    "        row = str(row).lower()          # lowercase (so that upper and lowercase words are treated the same)\n",
    "        row = word_tokenize(row)        # tokenize  (to perform lemmitization\n",
    "        row = [lemmatizer.lemmatize(word) if word not in words_not_changed else word for word in row]   # lemmatize\n",
    "\n",
    "    return str(row)                     # convert back to string\n",
    "\n",
    "\n",
    "df['description_lemmatized'] = df['description'].apply(preprocessing)\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "print()\n",
    "\n",
    "print(df.head())\n",
    "print()\n",
    "print('Number of labels per category:')\n",
    "print(df[hand_label].value_counts())\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       username                                        description hand.label  \\\n",
      "0   Casper30214  Army Civil Service(Retired);Military Ops Resea...      other   \n",
      "1         enckj  Former EPA Regional Administrator, President o...       acad   \n",
      "2  nuclearkelly  Scientist at ORNL, DOE Early Career Awardee, F...       acad   \n",
      "3       stukhan  Dad. Director of the Australian Graduate Schoo...       acad   \n",
      "4       PatMag7  Podcasting about Feminist Participatory Action...       acad   \n",
      "\n",
      "                              description_lemmatized  \n",
      "0  ['army', 'civil', 'service', '(', 'retire', ')...  \n",
      "1  ['former', 'epa', 'regional', 'administrator',...  \n",
      "2  ['scientist', 'at', 'ornl', ',', 'doe', 'early...  \n",
      "3  ['dad', '.', 'director', 'of', 'the', 'austral...  \n",
      "4  ['podcast', 'about', 'feminist', 'participator...  \n",
      "\n",
      "       username                                        description hand.label  \\\n",
      "0   Casper30214  Army Civil Service(Retired);Military Ops Resea...      other   \n",
      "1         enckj  Former EPA Regional Administrator, President o...       acad   \n",
      "2  nuclearkelly  Scientist at ORNL, DOE Early Career Awardee, F...       acad   \n",
      "3       stukhan  Dad. Director of the Australian Graduate Schoo...       acad   \n",
      "4       PatMag7  Podcasting about Feminist Participatory Action...       acad   \n",
      "\n",
      "                              description_lemmatized  \n",
      "0  ['army', 'civil', 'service', '(', 'retire', ')...  \n",
      "1  ['former', 'epa', 'regional', 'administrator',...  \n",
      "2  ['scientist', 'at', 'ornl', ',', 'doe', 'early...  \n",
      "3  ['dad', '.', 'director', 'of', 'the', 'austral...  \n",
      "4  ['podcast', 'about', 'feminist', 'participator...  \n",
      "\n",
      "Number of labels per category:\n",
      "hand.label\n",
      "other    7232\n",
      "media     397\n",
      "acad      187\n",
      "gov        36\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "tag_map['AS'] = wn.ADJ_SAT\n",
    "\n",
    "\n",
    "# filepath = \"finalized_8K_accounts.csv\"\n",
    "filepath = \"finalized_8K_accounts_emojis_replaced.csv\"\n",
    "hand_label = \"hand.label\"\n",
    "government = \"gov\"\n",
    "academia = \"acad\"\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "df = df[((df[hand_label]=='media') | (df[hand_label]== academia) | (df[hand_label]==government) | (df[hand_label]=='other' ))]\n",
    "\n",
    "\n",
    "df = df[['username','description',hand_label]] # keep only relevant columns\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_not_changed = ['media']\n",
    "\n",
    "def preprocessing(row):\n",
    "    if str(row) == \"nan\":\n",
    "        lemma = \"\"\n",
    "    else:\n",
    "        row = str(row).lower()\n",
    "        row = word_tokenize(row)        # tokenize\n",
    "        lemma = [lemmatizer.lemmatize(token, tag_map[tag[0]]) if token not in words_not_changed else token for token, tag in pos_tag(row)] # lemmatization, depending on part-of-speech\n",
    "        lemma = [\"\" if re.search(r'\\b[0-9]+\\b\\s*', lem) else lem for lem in lemma]  # removing\n",
    "    return str(lemma)\n",
    "\n",
    "df['description_lemmatized'] = df['description'].apply(preprocessing)\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "print()\n",
    "\n",
    "print(df.head())\n",
    "print()\n",
    "print('Number of labels per category:')\n",
    "print(df[hand_label].value_counts())\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          username                                        description  \\\n",
      "0  conserveturtles  STC is the oldest sea turtle conservation orga...   \n",
      "1  WhySharksMatter  Research associate at @ASU @ASUinDC studying s...   \n",
      "2     BenDiamondFL  Husband, Father and proud Floridian | Florida ...   \n",
      "3    NancyRichmond  Speaker 🗣️ | University Professor 📚  | Social ...   \n",
      "4       stemdotorg  Science 🔬 Technology 🛰 Engineering ⚙️ Math 📐 E...   \n",
      "\n",
      "  hand.label                             description_lemmatized  \n",
      "0       acad  ['stc', 'be', 'the', 'old', 'sea', 'turtle', '...  \n",
      "1       acad  ['research', 'associate', 'at', '@', 'asu', '@...  \n",
      "2       acad  ['husband', ',', 'father', 'and', 'proud', 'fl...  \n",
      "3       acad  ['speaker', '🗣️', '|', 'university', 'professo...  \n",
      "4       acad  ['science', '🔬', 'technology', '🛰', 'engineeri...  \n",
      "\n",
      "Number of labels per category:\n",
      "hand.label\n",
      "media    887\n",
      "acad     221\n",
      "gov       42\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enhanced data\n",
    "filepath = \"finalized_BIASED_accounts_ONLY_NON_OTHER.csv\"\n",
    "\n",
    "\n",
    "df2 = pd.read_csv(filepath)\n",
    "df2 = df2[((df2[hand_label]=='media') | (df2[hand_label]== academia) | (df2[hand_label]==government) | (df2[hand_label]=='other' ))]\n",
    "\n",
    "df2 = df2[['username','description',hand_label]] # keep only relevant columns\n",
    "\n",
    "\n",
    "df2['description_lemmatized'] = df2['description'].apply(preprocessing)\n",
    "\n",
    "print(df2.head())\n",
    "print()\n",
    "print('Number of labels per category:')\n",
    "print(df2[hand_label].value_counts())\n",
    "print()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# split my data into training, and test sets\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = df['description_lemmatized']\n",
    "y_labels = df[hand_label]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2, random_state=42, stratify=y_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "X2 = df2['description_lemmatized']\n",
    "Y2 = df2[hand_label]\n",
    "\n",
    "X_train = pd.concat([X_train, X2])\n",
    "y_train = pd.concat([y_train, Y2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE: (7431,)\n"
     ]
    }
   ],
   "source": [
    "# print(\"SHAPE:\", X_train.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description_lemmatized\n",
      "                                                                                                                                                                                                                                                       1358\n",
      "['teacher']                                                                                                                                                                                                                                               3\n",
      "['she/her']                                                                                                                                                                                                                                               3\n",
      "['']                                                                                                                                                                                                                                                      3\n",
      "['.']                                                                                                                                                                                                                                                     3\n",
      "                                                                                                                                                                                                                                                       ... \n",
      "['retire', 'engineer', '...']                                                                                                                                                                                                                             1\n",
      "['if', 'it', \"'s\", 'nfl', 'i', \"'m\", 'on', 'it', '.', 'big', 'buc', 'fan', '.', 'i', \"'m\", 'only', 'on', 'twitter', 'so', 'i', 'do', \"n't\", 'get', 'fin', '.', '#', 'bucsreport', '#', 'theprofessor', '#', 'whatshappeningnfl', '#', 'nfledition']       1\n",
      "['#', 'gobolts', '#', 'gobucs', '#', 'thankyoutom', '#', 'tombrady', 'you', 'get', 'one', 'life', ',', 'make', 'it', 'yours', '!']                                                                                                                        1\n",
      "['eh', '.', 'rts', 'a', 'a', 'coping', 'mechanism', '.', 'some', 'music', ',', 'some', 'tech', ',', 'some', 'programming', ',', 'some', 'random', '.', 'sea', 'marvel', '.']                                                                              1\n",
      "['behind', 'the', 'scene', 'at', 'in', 'fisherman', 'television']                                                                                                                                                                                         1\n",
      "Name: count, Length: 6481, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(X.value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description_lemmatized\n",
      "                                                                                                                                                                                                                                                                                                                                                                  1092\n",
      "['she/her']                                                                                                                                                                                                                                                                                                                                                          3\n",
      "['.']                                                                                                                                                                                                                                                                                                                                                                3\n",
      "[':', 'flag', ':', 'united', 'state', ':']                                                                                                                                                                                                                                                                                                                           2\n",
      "['']                                                                                                                                                                                                                                                                                                                                                                 2\n",
      "                                                                                                                                                                                                                                                                                                                                                                  ... \n",
      "['tv', 'news', 'photojournalist', 'with', 'wtsp-tv', '.', 'i', 'grow', 'up', 'in', 'virginia', '.', 'i', 'love', 'to', 'spend', 'time', 'with', 'family', ',', 'fishing', ',', 'kayaking', ',', 'travel', ',', 'gardening', 'and', 'cooking', '.']                                                                                                                   1\n",
      "[':', 'red', 'heart', ':', 'sara', \"'s\", 'husband', ':', 'deciduous', 'tree', ':', 'land', 'agent', 'and', 'investor', 'in', 'florida', ':', 'house', 'with', 'garden', ':', 'aspiring', 'builder', 'of', 'small', 'multifamily', 'property', ':', 'man', 'technologist', ':', 'medium-light', 'skin', 'tone', ':', 'web', 'designer', 'for', 'good', 'cause']       1\n",
      "['disciple', 'of', 'yeshua', '*', 'marry', '*', 'israel', ',', 'apple', 'of', 'god', \"'s\", 'eye', '*', 'if', 'you', 'truly', 'belong', 'to', 'him', 'love', 'for', 'jewish', 'peep', 'will', 'come', 'naturally', '*', 'm', 'columbia', 'u', '*', 'analytical', 'one']                                                                                               1\n",
      "['“', 'the', 'side', 'that', 'want', 'to', 'take', 'the', 'choice', 'away', 'from', 'woman', 'and', 'give', 'it', 'to', 'the', 'state', ',', 'they', '’', 're', 'fight', 'a', 'losing', 'battle', '.', 'time', 'be', 'on', 'the', 'side', 'of', 'change.', '”', '~rbg']                                                                                              1\n",
      "['husband', ',', 'dad', ',', 'minister', ',', 'journalist', ',', 'instructor', ',', 'uber', 'driver', ',', 'fisherman']                                                                                                                                                                                                                                              1\n",
      "Name: count, Length: 6328, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(X_train.value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand.label\n",
      "other    1447\n",
      "media      80\n",
      "acad       37\n",
      "gov         7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(y_test.value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# nan_counts = X_train.isna().sum()\n",
    "# print(nan_counts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# nan_counts = y_train.isna().sum()\n",
    "# print(nan_counts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\",ngram_range=(1,2) )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vectorizer', count_vectorizer),\n",
    "    ('transformer', tfidf_transformer),\n",
    "    ('normalize', StandardScaler(with_mean=False)),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "tfidf_param_grid = [\n",
    "    {\n",
    "        ''\n",
    "        'vectorizer__min_df': [0.0],\n",
    "        'transformer__use_idf': [True],\n",
    "        # 'normalize__with_mean': [False],\n",
    "        'classifier__C': [1.0e-10, 0.5, 3.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'classifier__class_weight': [\"balanced\"]\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=\"english\",ngram_range=(1,2) )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n14877 1 1\\n\\n52688 2 2\\n\\n61509 3 3\\n'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "# X = vectorizer.fit_transform(X_train)\n",
    "# print(len(vectorizer.get_feature_names_out()))\n",
    "\"\"\"\n",
    "14877 1 1\n",
    "\n",
    "52688 2 2\n",
    "\n",
    "61509 3 3\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "bag_of_words_pipeline = Pipeline([\n",
    "    ('vectorizer', count_vectorizer),\n",
    "    ('normalize', StandardScaler(with_mean=False)),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "bag_of_words_param_grid = [\n",
    "    {\n",
    "        'vectorizer__min_df': [0.0],\n",
    "        # 'normalize__with_mean': [False],\n",
    "        'classifier__C': [1.0e-10, 0.5, 3.0, 10.0],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'classifier__class_weight': [\"balanced\"]\n",
    "    }\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    }
   ],
   "source": [
    "# Grid Search - TF-IDF\n",
    "tfidf_grid_search = GridSearchCV(estimator=tfidf_pipeline, param_grid=tfidf_param_grid, cv=5, scoring='accuracy', verbose=1, error_score=\"raise\")\n",
    "\n",
    "tfidf_grid_search.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bag Of Words\n",
    "bag_of_words_grid_search = GridSearchCV(estimator=bag_of_words_pipeline, param_grid=bag_of_words_param_grid, cv=5, scoring='accuracy', verbose=1,error_score=\"raise\" )\n",
    "\n",
    "bag_of_words_grid_search.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidf_best_hyperparameters = tfidf_grid_search.best_params_\n",
    "# print(\"Best TF-IDF SVM ACCURACY:\", tfidf_grid_search.best_score_)\n",
    "# print(\"Best TF-IDF SVM Hyperparameters:\", tfidf_best_hyperparameters)\n",
    "\n",
    "# print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bag_of_words_best_hyperparameters = bag_of_words_grid_search.best_params_\n",
    "# print(\"Best TF-IDF Bag of Words ACCURACY:\", bag_of_words_grid_search.best_score_)\n",
    "# print(\"Best TF-IDF Bag of Words Hyperparameters:\", bag_of_words_best_hyperparameters)\n",
    "\n",
    "# print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(tfidf_grid_search.cv_results_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidf_best_SVM_model = tfidf_grid_search.best_estimator_\n",
    "tfidf_pipeline.set_params(**tfidf_grid_search.best_params_)\n",
    "tfidf_pipeline.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bag_of_words_best_SVM_model = bag_of_words_grid_search.best_estimator_\n",
    "bag_of_words_pipeline.set_params(**bag_of_words_grid_search.best_params_)\n",
    "bag_of_words_pipeline.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cross validate\n",
    "y_pred_tfidf = cross_val_predict(tfidf_best_SVM_model, X_train, y_train, cv=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cross validate\n",
    "\n",
    "y_pred_bag_of_words = cross_val_predict(bag_of_words_best_SVM_model, X_train, y_train, cv=5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_train, y_pred_tfidf, normalize='true')\n",
    "\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['academia', 'government', 'media', 'other'])\n",
    "# disp.plot()\n",
    "\n",
    "# plt.title(\"TF-IDF Train\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_train, y_pred_bag_of_words, normalize='true')\n",
    "\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['academia', 'government', 'media', 'other'])\n",
    "#disp.plot()\n",
    "\n",
    "# plt.title(\"Bag Of Words Train\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(\"TF-IDF Classification Report:\")\n",
    "# print(metrics.classification_report(y_train, y_pred_tfidf))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(\"Bag of Words Classification Report:\")\n",
    "# print(metrics.classification_report(y_train, y_pred_bag_of_words))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidf_y_pred_test = tfidf_pipeline.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bag_of_words_y_pred_test = bag_of_words_pipeline.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"TF-IDF Classification Report TEST:\")\n",
    "print(metrics.classification_report(y_test, tfidf_y_pred_test))\n",
    "print()\n",
    "\n",
    "print(\"Bag of Words Classification Report TEST:\")\n",
    "print(metrics.classification_report(y_test, bag_of_words_y_pred_test))\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----------------------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, tfidf_y_pred_test, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['academia', 'government', 'media', 'other'])\n",
    "disp.plot()\n",
    "\n",
    "plt.title(\"TF-IDF TEST SET\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, bag_of_words_y_pred_test, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['academia', 'government', 'media', 'other'])\n",
    "disp.plot()\n",
    "\n",
    "plt.title(\"Bag of Words TEST SET\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"TF-IDF Classification Report TEST:\")\n",
    "print(metrics.classification_report(y_test, tfidf_y_pred_test))\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Bag of Words Classification Report TEST:\")\n",
    "print(metrics.classification_report(y_test, bag_of_words_y_pred_test))\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\"\"\"\n",
    "For both SVM and NaiveBayes :\n",
    "\n",
    "Run this on Bigrams:\n",
    "ngram_range(1,1) (2,2) (3,3)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Done.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "No Weights:\n",
    "---------------------------------------------------------------\n",
    "(1,1)\n",
    "Bag of Words Classification Report TEST:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        acad       0.88      0.19      0.31        37\n",
    "         gov       0.00      0.00      0.00         7\n",
    "       media       0.79      0.46      0.58        80\n",
    "       other       0.95      0.99      0.97      1447\n",
    "\n",
    "    accuracy                           0.94      1571\n",
    "   macro avg       0.65      0.41      0.47      1571\n",
    "weighted avg       0.93      0.94      0.93      1571\n",
    "\n",
    "TF-IDF Classification Report TEST:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        acad       0.73      0.22      0.33        37\n",
    "         gov       0.00      0.00      0.00         7\n",
    "       media       0.88      0.44      0.58        80\n",
    "       other       0.95      1.00      0.97      1447\n",
    "\n",
    "    accuracy                           0.94      1571\n",
    "   macro avg       0.64      0.41      0.47      1571\n",
    "weighted avg       0.93      0.94      0.93      1571\n",
    "\n",
    "---------------------------------------------------------------\n",
    "ngram range Unigrams and Bigrams : (1,2)\n",
    "\n",
    "Bag of Words Classification Report TEST:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        acad       1.00      0.03      0.05        37\n",
    "         gov       1.00      0.14      0.25         7\n",
    "       media       1.00      0.12      0.22        80\n",
    "       other       0.93      1.00      0.96      1447\n",
    "\n",
    "    accuracy                           0.93      1571\n",
    "   macro avg       0.98      0.32      0.37      1571\n",
    "weighted avg       0.93      0.93      0.90      1571\n",
    "\n",
    "TF-IDF Classification Report TEST:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        acad       0.80      0.11      0.19        37\n",
    "         gov       1.00      0.14      0.25         7\n",
    "       media       0.96      0.30      0.46        80\n",
    "       other       0.94      1.00      0.97      1447\n",
    "\n",
    "    accuracy                           0.94      1571\n",
    "   macro avg       0.92      0.39      0.47      1571\n",
    "weighted avg       0.94      0.94      0.92      1571\n",
    "\n",
    "---------------------------------------------------------------\n",
    "(2,2)\n",
    "Bag of Words Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        acad       0.00      0.00      0.00       150\n",
    "         gov       0.00      0.00      0.00        29\n",
    "       media       1.00      0.06      0.11       317\n",
    "       other       0.92      1.00      0.96      5785\n",
    "\n",
    "    accuracy                           0.92      6281\n",
    "   macro avg       0.48      0.26      0.27      6281\n",
    "weighted avg       0.90      0.92      0.89      6281\n",
    "\n",
    "TF-IDF Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        acad       0.47      0.09      0.16       150\n",
    "         gov       0.00      0.00      0.00        29\n",
    "       media       0.76      0.28      0.41       317\n",
    "       other       0.94      0.99      0.96      5785\n",
    "\n",
    "    accuracy                           0.93      6281\n",
    "   macro avg       0.54      0.34      0.38      6281\n",
    "weighted avg       0.91      0.93      0.91      6281\n",
    "\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "Weighted:\n",
    "\n",
    "---------------------------------------------------------------\n",
    "(1,1)\n",
    "Bag of Words Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        acad       0.65      0.34      0.45       150\n",
    "         gov       0.25      0.14      0.18        29\n",
    "       media       0.77      0.47      0.58       317\n",
    "       other       0.95      0.99      0.97      5785\n",
    "\n",
    "    accuracy                           0.94      6281\n",
    "   macro avg       0.66      0.48      0.54      6281\n",
    "weighted avg       0.93      0.94      0.93      6281\n",
    "\n",
    "\n",
    "TF-IDF Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        acad       0.72      0.26      0.38       150\n",
    "         gov       0.36      0.14      0.20        29\n",
    "       media       0.82      0.47      0.60       317\n",
    "       other       0.95      0.99      0.97      5785\n",
    "\n",
    "    accuracy                           0.94      6281\n",
    "   macro avg       0.71      0.47      0.54      6281\n",
    "weighted avg       0.93      0.94      0.93      6281\n",
    "\n",
    "---------------------------------------------------------------\n",
    "(2,2)\n",
    "\n",
    "Bag of Words Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        acad       0.00      0.00      0.00       150\n",
    "         gov       0.00      0.00      0.00        29\n",
    "       media       1.00      0.06      0.11       317\n",
    "       other       0.92      1.00      0.96      5785\n",
    "\n",
    "    accuracy                           0.92      6281\n",
    "   macro avg       0.48      0.26      0.27      6281\n",
    "weighted avg       0.90      0.92      0.89      6281\n",
    "\n",
    "\n",
    "TF-IDF Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        acad       0.47      0.09      0.16       150\n",
    "         gov       0.00      0.00      0.00        29\n",
    "       media       0.76      0.28      0.41       317\n",
    "       other       0.94      0.99      0.96      5785\n",
    "\n",
    "    accuracy                           0.93      6281\n",
    "   macro avg       0.54      0.34      0.38      6281\n",
    "weighted avg       0.91      0.93      0.91      6281\n",
    "---------------------------------------------------------------\n",
    "\n",
    "compare F-1 scores for specifics:\n",
    "based on just f1- accuracy:\n",
    "the one with the best F-1 scores in weighted SVM\n",
    "the one with the best F-1 scores in unweighted SVM\n",
    "\n",
    "\n",
    "\n",
    "what is best NB or SVM\n",
    "TF-IDF VS BOW\n",
    "\n",
    "uni vs bi grams\n",
    "uni weighted vs uni unweighted\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Done.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
