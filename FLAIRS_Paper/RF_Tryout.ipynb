{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19b8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, BERTTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "tag_map['AS'] = wn.ADJ_SAT\n",
    "\n",
    "# filepath = \"finalized_8K_accounts.csv\"\n",
    "# filepath = \"finalized_8K_accounts_emojis_replaced.csv\"\n",
    "# filepath = \"FINALIZED_Training_Data_ALL_Available_Descriptions_EMOJIS_REPLACED.csv\"\n",
    "filepath = \"FINALIZED_Training_Data_ALL_Available_Descriptions_EMOJIS_UNCHANGED.csv\"\n",
    "\n",
    "hand_label = \"hand.label_simplified\"\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Removing all the \"-int\" (international, non-English, descriptions)\n",
    "#dict.fromkeys(df[hand_label])\n",
    "df = df[((df[hand_label] == 'media') | (df[hand_label] == 'tourbiz') |(df[hand_label] == 'acad') | (df[hand_label] == 'gov') | (\n",
    "        df[hand_label] == 'other'))]\n",
    "\n",
    "df = df[['username', 'description', hand_label]]  # keep only relevant columns\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_not_changed = ['media']\n",
    "\n",
    "# Lemmatization (preprocessing)\n",
    "def preprocessing(row):\n",
    "    if str(row) == \"nan\":\n",
    "        lemma = \"\"\n",
    "    else:\n",
    "        row = str(row).lower()\n",
    "        row = word_tokenize(row)  # tokenize\n",
    "        lemma = [lemmatizer.lemmatize(token, tag_map[tag[0]]) if token not in words_not_changed else token for\n",
    "                 token, tag in pos_tag(row)]  # lemmatization, depending on part-of-speech\n",
    "        lemma = [\"\" if re.search(r'\\b[0-9]+\\b\\s*', lem) else lem for lem in lemma]  # removing\n",
    "    return str(lemma)\n",
    "\n",
    "\n",
    "df['description_lemmatized'] = df['description'].apply(preprocessing)\n",
    "\n",
    "# Remove all the empty descriptions\n",
    "df = df[df['description_lemmatized'] != \"\"]\n",
    "#df[hand_label]\n",
    "#print(df.shape)\n",
    "#df[df['description_lemmatized'] != \"\"].shape\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Smaller, faster model (maybe not as good)\n",
    "# model = SentenceTransformer('all-mpnet-base-v2') # Larger, 5x slower model (best performance, supposedly)\n",
    "\n",
    "\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "#print(type(df[['description']]))\n",
    "# embeddings = model.encode(df['description'].tolist())  # If we use the NON-preprocessed text (typical for BERT)\n",
    "embeddings = model.encode(df['description'].tolist())  # If we use the LEMMATIZED, hence PREPROCESSED, text (not as typical for BERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5164f668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11923, 384)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af00d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "\n",
      "\n",
      "RF UNWEIGHT ENHANCED BEST PARAMS: {'classifier__bootstrap': True, 'classifier__criterion': 'gini', 'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200}\n",
      "\n",
      "\n",
      "\n",
      "CV confusion matrix of predictions:\n",
      "\n",
      "[[4.65116279e-03 0.00000000e+00 9.30232558e-03 9.86046512e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 5.82524272e-02 9.41747573e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 4.43418014e-01 5.56581986e-01\n",
      "  0.00000000e+00]\n",
      " [1.32432790e-04 0.00000000e+00 6.75407231e-03 9.93113495e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.58064516e-02 9.74193548e-01\n",
      "  0.00000000e+00]]\n",
      "\n",
      "\n",
      "\n",
      "CV metrics summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        acad       0.67      0.00      0.01       430\n",
      "         gov       0.00      0.00      0.00       103\n",
      "       media       0.90      0.44      0.59      1299\n",
      "       other       0.84      0.99      0.91      7551\n",
      "     tourbiz       0.00      0.00      0.00       155\n",
      "\n",
      "    accuracy                           0.85      9538\n",
      "   macro avg       0.48      0.29      0.30      9538\n",
      "weighted avg       0.82      0.85      0.80      9538\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test set metrics summary:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        acad       1.00      0.02      0.04       108\n",
      "         gov       0.00      0.00      0.00        25\n",
      "       media       0.91      0.45      0.60       325\n",
      "       other       0.84      0.99      0.91      1888\n",
      "     tourbiz       1.00      0.03      0.05        39\n",
      "\n",
      "    accuracy                           0.85      2385\n",
      "   macro avg       0.75      0.30      0.32      2385\n",
      "weighted avg       0.85      0.85      0.81      2385\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# split my data into training, and test sets\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# X = df['description_lemmatized']\n",
    "X = embeddings\n",
    "y_labels = df[hand_label]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2, random_state=42, stratify=y_labels)\n",
    "\n",
    "# X2 = df2['description_lemmatized']\n",
    "# Y2 = df2[hand_label]\n",
    "\n",
    "# X_train = pd.concat([X_train, X2])\n",
    "# y_train = pd.concat([y_train, Y2])\n",
    "\n",
    "\n",
    "\n",
    "# BERT_transformer = BERTTransformer()\n",
    "\n",
    "# n_gram_ranges = [(1,1), (1,2), (2,2)]\n",
    "n_gram_ranges = [(1,1)]\n",
    "\n",
    "result = {}\n",
    "result_cv = {}\n",
    "\n",
    "for n_gram_range in n_gram_ranges:\n",
    "    # count_vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=n_gram_range)\n",
    "    BERT_pipeline = Pipeline([\n",
    "        # ('vectorizer', count_vectorizer),\n",
    "        # ('transformer', BERT_transformer),\n",
    "        #('normalize', StandardScaler(with_mean=False)),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    print(n_gram_range)\n",
    "    \n",
    "    max_depth_vals = [int(x) for x in np.linspace(10, 100, num = 3)]\n",
    "    max_depth_vals.append(None)\n",
    "    \n",
    "   # {'classifier__bootstrap': True, \n",
    "   #  'classifier__criterion': 'gini', \n",
    "   #  'classifier__max_depth': None, \n",
    "   #  'classifier__min_samples_leaf': 1, \n",
    "    # 'classifier__min_samples_split': 5, \n",
    "   #  'classifier__n_estimators': 200, '\n",
    "   #  vectorizer__min_df': 5}\n",
    "    \n",
    "    BERT_param_grid = [\n",
    "        {\n",
    "            # 'vectorizer__min_df': [1, 2, 5],\n",
    "            # 'transformer__use_idf': [True],\n",
    "            \n",
    "            'classifier__n_estimators': [200], # [int(x) for x in np.linspace(start = 200, stop = 200, num = 1)],\n",
    "            'classifier__criterion': ['gini'],\n",
    "            'classifier__max_depth': [None],\n",
    "            'classifier__min_samples_split': [2],\n",
    "            'classifier__min_samples_leaf': [1],\n",
    "            'classifier__bootstrap': [True]\n",
    "           # 'classifier__class_weight': [\"balanced\"]\n",
    "            \n",
    "            \n",
    "            # 'classifier__n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 4)],\n",
    "            #'classifier__criterion': ['gini'],\n",
    "            #'classifier__max_depth': max_depth_vals,\n",
    "            #'classifier__min_samples_split': [2,5,10],\n",
    "            #'classifier__min_samples_leaf': [1,2,4],\n",
    "            #'classifier__bootstrap': [True],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # !!! Does STRATIFICATION BY DEFAULT !!!\n",
    "    BERT_grid_search = GridSearchCV(estimator=BERT_pipeline, param_grid=BERT_param_grid, cv=3,\n",
    "                                            scoring='accuracy', verbose=1, error_score=\"raise\")\n",
    "    BERT_grid_search.fit(X_train, y_train)\n",
    "    BERT_best_hyperparameters = BERT_grid_search.best_params_\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"RF UNWEIGHT ENHANCED BEST PARAMS:\", BERT_best_hyperparameters)\n",
    "\n",
    "    BERT_best_RF_model = BERT_grid_search.best_estimator_\n",
    "    BERT_pipeline.set_params(**BERT_grid_search.best_params_)\n",
    "    BERT_pipeline.fit(X_train, y_train)\n",
    "    y_pred_BERT_cross_validation = cross_val_predict(BERT_best_RF_model, X_train, y_train, cv=3)\n",
    "    BERT_y_pred_test = BERT_pipeline.predict(X_test)\n",
    "\n",
    "    cm_count = confusion_matrix(y_train, y_pred_BERT_cross_validation, normalize='true')\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"CV confusion matrix of predictions:\")\n",
    "    print()\n",
    "    print(cm_count)\n",
    "    \n",
    "    # np.savetxt(\"RF_BERT_unweighted_enhanced_cross_validation_confusion_matrix\" + str(n_gram_range) + '.txt', cm_count,\n",
    "    #            delimiter=',', fmt='%f')\n",
    "\n",
    "    result_cv[\"RF_BERT_unweighted_enhanced_predictions_CV\" + str(n_gram_range)] = metrics.classification_report(y_train, y_pred_BERT_cross_validation)\n",
    "\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"CV metrics summary:\")\n",
    "    print(result_cv[\"RF_BERT_unweighted_enhanced_predictions_CV\" + str(n_gram_range)])\n",
    "    \n",
    "    result[\"RF_BERT_unweighted_enhanced_predictions_testSet\" + str(n_gram_range)] = metrics.classification_report(y_test, BERT_y_pred_test)\n",
    "                         \n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Test set metrics summary:\")\n",
    "    print()\n",
    "    print(result[\"RF_BERT_unweighted_enhanced_predictions_testSet\" + str(n_gram_range)])\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print()                                                                                       \n",
    "                                                                                                                \n",
    "    # filename = 'RF_BERT_unweighted_enhanced_model' + str(n_gram_range) + '.pickle'\n",
    "    # save model\n",
    "    # pickle.dump(BERT_pipeline, open(filename, \"wb\"))\n",
    "\n",
    "    # full_x = pd.concat([X_train, X_test])\n",
    "    # full_y = pd.concat([y_train, y_test])\n",
    "\n",
    "    # BERT_grid_search.fit(full_x, full_y)\n",
    "    BERT_pipeline.set_params(**BERT_grid_search.best_params_)\n",
    "    # BERT_pipeline.fit(full_x, full_y)\n",
    "    BERT_pipeline.fit(X, y_labels)\n",
    "\n",
    "    #filename = 'RF_BERT_unweighted_enhanced_model_full' + str(n_gram_range) + '.pickle'\n",
    "    #pickle.dump(BERT_pipeline, open(filename, \"wb\"))\n",
    "    \n",
    "    # def save_dict_to_file(dictionary, filename):\n",
    "    #\twith open(filename, 'w') as file:\n",
    "    #    \tjson.dump(dictionary, file)\n",
    "    #\n",
    "    # save_dict_to_file(result[\"RF_BERT_unweighted_enhanced_predictions_testSet\" + str(n_gram_range)], 'RF_BERT_unWeighted_full' + str(n_gram_range) + '.txt')\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0c18e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
