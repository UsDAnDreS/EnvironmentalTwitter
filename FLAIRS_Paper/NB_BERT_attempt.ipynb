{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0eb8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, BERTTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Initialization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# count_vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1, 1))\n",
    "# BERT_transformer = BERTTransformer()\n",
    "nb_clf = MultinomialNB()\n",
    "\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "tag_map['AS'] = wn.ADJ_SAT\n",
    "\n",
    "# filepath = \"finalized_8K_accounts.csv\"\n",
    "# filepath = \"finalized_8K_accounts_emojis_replaced.csv\"\n",
    "# filepath = \"FINALIZED_Training_Data_ALL_Available_Descriptions_EMOJIS_REPLACED.csv\"\n",
    "filepath = \"FINALIZED_Training_Data_ALL_Available_Descriptions_EMOJIS_UNCHANGED.csv\"\n",
    "\n",
    "hand_label = \"hand.label_simplified\"\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Removing all the \"-int\" (international, non-English, descriptions)\n",
    "#dict.fromkeys(df[hand_label])\n",
    "df = df[((df[hand_label] == 'media') | (df[hand_label] == 'tourbiz') |(df[hand_label] == 'acad') | (df[hand_label] == 'gov') | (\n",
    "        df[hand_label] == 'other'))]\n",
    "\n",
    "df = df[['username', 'description', hand_label]]  # keep only relevant columns\n",
    "\n",
    "words_not_changed = ['media']\n",
    "\n",
    "result = {}\n",
    "n_gram_range = (1, 1)\n",
    "\n",
    "\n",
    "def preprocessing(row):\n",
    "    if str(row) == \"nan\":\n",
    "        lemma = \"\"\n",
    "    else:\n",
    "        row = str(row).lower()\n",
    "        row = word_tokenize(row)  # tokenize\n",
    "        lemma = [lemmatizer.lemmatize(token, tag_map[tag[0]]) if token not in words_not_changed else token for\n",
    "                 token, tag in pos_tag(row)]  # lemmatization, depending on part-of-speech\n",
    "        lemma = [\"\" if re.search(r'\\b[0-9]+\\b\\s*', lem) else lem for lem in lemma]  # removing\n",
    "    return str(lemma)\n",
    "\n",
    "\n",
    "df['description_lemmatized'] = df['description'].apply(preprocessing)\n",
    "\n",
    "# Remove all the empty descriptions\n",
    "df = df[df['description_lemmatized'] != \"\"]\n",
    "#df[hand_label]\n",
    "#print(df.shape)\n",
    "#df[df['description_lemmatized'] != \"\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239730aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "#print(type(df[['description']]))\n",
    "embeddings = model.encode(df['description'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e4ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'BERT_df.pickle'\n",
    "pickle.dump(df, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d229f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'BERT_embeddings.pickle'\n",
    "pickle.dump(embeddings, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a62aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('BERT_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7aa47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings.to_csv('BERT_embeddings.csv', index=False)\n",
    "np.savetxt(\"BERT_embeddings.csv\", embeddings, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a523a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.14470369,  0.40523018, ...,  0.1269892 ,\n",
       "         0.09070657,  0.10037136],\n",
       "       [ 0.14470369,  1.        ,  0.19039135, ...,  0.10073964,\n",
       "         0.07227026, -0.04743898],\n",
       "       [ 0.40523018,  0.19039135,  1.        , ...,  0.16159729,\n",
       "         0.10458214,  0.0318919 ],\n",
       "       ...,\n",
       "       [ 0.1269892 ,  0.10073964,  0.16159729, ...,  1.        ,\n",
       "         0.11394472,  0.2495047 ],\n",
       "       [ 0.09070657,  0.07227026,  0.10458214, ...,  0.11394472,\n",
       "         1.        ,  0.02802738],\n",
       "       [ 0.10037136, -0.04743898,  0.0318919 , ...,  0.2495047 ,\n",
       "         0.02802738,  1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings\n",
    "np.corrcoef(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f8f5bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(1, 1)\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "\n",
      "\n",
      "NAIVE BAYES UNWEIGHTED ENHANCED BEST PARAMS: {'classifier__alpha': 1e-10, 'classifier__fit_prior': True}\n",
      "['other' 'other' 'other' ... 'other' 'other' 'other']\n",
      "\n",
      "\n",
      "\n",
      "CV confusion matrix of predictions:\n",
      "\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.21632025e-01 8.78367975e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 9.27029533e-04 9.99072970e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CV metrics summary:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        acad       0.00      0.00      0.00       430\n",
      "         gov       0.00      0.00      0.00       103\n",
      "       media       0.96      0.12      0.22      1299\n",
      "       other       0.80      1.00      0.89      7551\n",
      "     tourbiz       0.00      0.00      0.00       155\n",
      "\n",
      "    accuracy                           0.81      9538\n",
      "   macro avg       0.35      0.22      0.22      9538\n",
      "weighted avg       0.77      0.81      0.74      9538\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test set metrics summary:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        acad       0.00      0.00      0.00       108\n",
      "         gov       0.00      0.00      0.00        25\n",
      "       media       0.97      0.09      0.16       325\n",
      "       other       0.80      1.00      0.89      1888\n",
      "     tourbiz       0.00      0.00      0.00        39\n",
      "\n",
      "    accuracy                           0.80      2385\n",
      "   macro avg       0.35      0.22      0.21      2385\n",
      "weighted avg       0.77      0.80      0.73      2385\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrey/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# split my data into training, and test sets\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# X = df['description_lemmatized']\n",
    "X = embeddings\n",
    "y_labels = df[hand_label]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2, random_state=42, stratify=y_labels)\n",
    "\n",
    "# X2 = df2['description_lemmatized']\n",
    "# Y2 = df2[hand_label]\n",
    "#\n",
    "# X_train = pd.concat([X_train, X2])\n",
    "# y_train = pd.concat([y_train, Y2])\n",
    "\n",
    "\n",
    "# n_gram_ranges = [(1,1), (1,2), (2,2)]\n",
    "n_gram_ranges = [(1,1)]\n",
    "\n",
    "result = {}\n",
    "result_cv={}\n",
    "\n",
    "for n_gram_range in n_gram_ranges:\n",
    "    # count_vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=n_gram_range)\n",
    "\n",
    "    nb_BERT_pipeline = Pipeline([\n",
    "    \t# ('vectorizer', count_vectorizer),\n",
    "    \t# ('transformer', BERT_transformer),\n",
    "        ('normalize', MinMaxScaler()),\n",
    "    \t('classifier', nb_clf)\n",
    "    \t])\n",
    "\t\n",
    "    print()\n",
    "    print()\n",
    "    print(n_gram_range)\n",
    "\n",
    "    param_BERT_grid = [\n",
    "    \t{\n",
    "    \t    # 'vectorizer__min_df': [1,2,5],\n",
    "    \t    # 'transformer__use_idf': [True],\n",
    "    \t    'classifier__alpha': [1.0e-10, 0.5, 2.0, 5.0, 10.0],\n",
    "    \t    'classifier__fit_prior': [True, False],\n",
    "    \t}\n",
    "    ]\n",
    "\n",
    "    # !!! Does STRATIFICATION BY DEFAULT !!!\n",
    "    grid_search_BERT = GridSearchCV(nb_BERT_pipeline, param_BERT_grid, cv=10, scoring='accuracy', verbose=1)\n",
    "    grid_search_BERT.fit(X_train, y_train)\n",
    "    nb_BERT_best_hyperparameters = grid_search_BERT.best_params_\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"NAIVE BAYES UNWEIGHTED ENHANCED BEST PARAMS:\", grid_search_BERT.best_params_)\n",
    "    \n",
    "    nb_BERT_best_model =  grid_search_BERT.best_estimator_\n",
    "    nb_BERT_pipeline.set_params(**grid_search_BERT.best_params_)\n",
    "    nb_BERT_pipeline.fit(X_train, y_train)\n",
    "    y_pred_BERT_cross_validation = cross_val_predict(nb_BERT_pipeline, X_train, y_train, cv=10)\n",
    "    print(y_pred_BERT_cross_validation)\n",
    "\n",
    "    y_pred_BERT_test = nb_BERT_pipeline.predict(X_test)\n",
    "\n",
    "    cm_BERT = confusion_matrix(y_train, y_pred_BERT_cross_validation, normalize='true')\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"CV confusion matrix of predictions:\")\n",
    "    print()\n",
    "    print(cm_BERT)\n",
    "    \n",
    "    # np.savetxt(\"NB_BERT_unweighted_enhanced_cross_validation_confusion_matrix\" + str(n_gram_range) + '.txt', cm_BERT,\n",
    "    #           delimiter=',', fmt='%f')\n",
    "\n",
    "\n",
    "    result_cv[\"NB_BERT_unweighted_enhanced_predictions_CV\" + str(n_gram_range)] = metrics.classification_report(y_train, y_pred_BERT_cross_validation)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"CV metrics summary:\")\n",
    "    print(result_cv[\"NB_BERT_unweighted_enhanced_predictions_CV\" + str(n_gram_range)])\n",
    "    \n",
    "    \n",
    "    result[\"NB_BERT_unweighted_enhanced_predictions_testSet\" + str(n_gram_range)] = metrics.classification_report(y_test, y_pred_BERT_test)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Test set metrics summary:\")\n",
    "    print()\n",
    "    print(result[\"NB_BERT_unweighted_enhanced_predictions_testSet\" + str(n_gram_range)])\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    filename = 'NB_BERT_unweighted_enhanced_model' + str(n_gram_range) + '.pickle'\n",
    "    # save model\n",
    "    pickle.dump(nb_BERT_pipeline, open(filename, \"wb\"))\n",
    "\n",
    "    # full_x = pd.concat([X_train, X_test])\n",
    "    # full_y = pd.concat([y_train, y_test])\n",
    "\n",
    "    # bag_of_words_grid_search.fit(full_x, full_y)\n",
    "    nb_BERT_pipeline.set_params(**grid_search_BERT.best_params_)\n",
    "    # nb_BERT_pipeline.fit(full_x, full_y)\n",
    "    nb_BERT_pipeline.fit(X, y_labels)\n",
    "\n",
    "    filename = 'NB_BERT_unweighted_enhanced_model_full' + str(n_gram_range) + '.pickle'\n",
    "    pickle.dump(nb_BERT_pipeline, open(filename, \"wb\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a058f69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
