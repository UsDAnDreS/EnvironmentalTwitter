{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a90b7a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38302/2722022975.py:61: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df2 = df2[((df2[hand_label] == 'media') | (df[hand_label] == tourBiz) | (df2[hand_label] == academia) | (df2[hand_label] == government) | (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "SVM UNWEIGHT ENHANCED BEST PARAMS: {'classifier__C': 3.0, 'classifier__kernel': 'sigmoid', 'vectorizer__min_df': 5}\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "{'SVM_BOW_unweighted_enhanced_predictions_testSet(1, 1)': '              precision    recall  f1-score   support\\n\\n        acad       0.52      0.41      0.45        37\\n         gov       0.33      0.29      0.31         7\\n       media       0.62      0.69      0.65        80\\n       other       0.96      0.96      0.96      1447\\n     tourbiz       0.33      0.12      0.18         8\\n\\n    accuracy                           0.93      1579\\n   macro avg       0.55      0.49      0.51      1579\\nweighted avg       0.93      0.93      0.93      1579\\n'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "tag_map['AS'] = wn.ADJ_SAT\n",
    "\n",
    "# filepath = \"finalized_8K_accounts.csv\"\n",
    "filepath = \"finalized_8K_accounts_emojis_replaced.csv\"\n",
    "hand_label = \"hand.label\"\n",
    "government = \"gov\"\n",
    "academia = \"acad\"\n",
    "tourBiz = \"tourbiz\"\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "df = df[((df[hand_label] == 'media') | (df[hand_label] == tourBiz) |(df[hand_label] == academia) | (df[hand_label] == government) | (\n",
    "        df[hand_label] == 'other'))]\n",
    "\n",
    "df = df[['username', 'description', hand_label]]  # keep only relevant columns\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_not_changed = ['media']\n",
    "\n",
    "\n",
    "def preprocessing(row):\n",
    "    if str(row) == \"nan\":\n",
    "        lemma = \"\"\n",
    "    else:\n",
    "        row = str(row).lower()\n",
    "        row = word_tokenize(row)  # tokenize\n",
    "        lemma = [lemmatizer.lemmatize(token, tag_map[tag[0]]) if token not in words_not_changed else token for\n",
    "                 token, tag in pos_tag(row)]  # lemmatization, depending on part-of-speech\n",
    "        lemma = [\"\" if re.search(r'\\b[0-9]+\\b\\s*', lem) else lem for lem in lemma]  # removing\n",
    "    return str(lemma)\n",
    "\n",
    "\n",
    "df['description_lemmatized'] = df['description'].apply(preprocessing)\n",
    "\n",
    "# Enhanced data\n",
    "filepath = \"finalized_BIASED_accounts_ONLY_NON_OTHER_emojis_replaced.csv\"\n",
    "\n",
    "df2 = pd.read_csv(filepath)\n",
    "df2 = df2[((df2[hand_label] == 'media') | (df[hand_label] == tourBiz) | (df2[hand_label] == academia) | (df2[hand_label] == government) | (\n",
    "        df2[hand_label] == 'other'))]\n",
    "\n",
    "df2 = df2[['username', 'description', hand_label]]  # keep only relevant columns\n",
    "\n",
    "df2['description_lemmatized'] = df2['description'].apply(preprocessing)\n",
    "\n",
    "# split my data into training, and test sets\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = df['description_lemmatized']\n",
    "y_labels = df[hand_label]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2, random_state=42, stratify=y_labels)\n",
    "\n",
    "X2 = df2['description_lemmatized']\n",
    "Y2 = df2[hand_label]\n",
    "\n",
    "X_train = pd.concat([X_train, X2])\n",
    "y_train = pd.concat([y_train, Y2])\n",
    "\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "n_gram_ranges = [(1, 1)]\n",
    "\n",
    "result = {}\n",
    "for n_gram_range in n_gram_ranges:\n",
    "    count_vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=n_gram_range)\n",
    "    bag_of_words_pipeline = Pipeline([\n",
    "        ('vectorizer', count_vectorizer),\n",
    "        ('normalize', StandardScaler(with_mean=False)),\n",
    "        ('classifier', SVC(probability=True))\n",
    "    ])\n",
    "\n",
    "    bag_of_words_param_grid = [\n",
    "        {\n",
    "            'vectorizer__min_df': [1, 2, 5],\n",
    "            'classifier__C': [1.0e-10, 0.5, 3.0, 10.0],\n",
    "            'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    bag_of_words_grid_search = GridSearchCV(estimator=bag_of_words_pipeline, param_grid=bag_of_words_param_grid, cv=5,\n",
    "                                            scoring='accuracy', verbose=1, error_score=\"raise\")\n",
    "    bag_of_words_grid_search.fit(X_train, y_train)\n",
    "    bag_of_words_best_hyperparameters = bag_of_words_grid_search.best_params_\n",
    "    print(\"SVM UNWEIGHT ENHANCED BEST PARAMS:\", bag_of_words_best_hyperparameters)\n",
    "\n",
    "    bag_of_words_best_SVM_model = bag_of_words_grid_search.best_estimator_\n",
    "    bag_of_words_pipeline.set_params(**bag_of_words_grid_search.best_params_)\n",
    "    bag_of_words_pipeline.fit(X_train, y_train)\n",
    "    y_pred_bag_of_words_cross_validation = cross_val_predict(bag_of_words_best_SVM_model, X_train, y_train, cv=5)\n",
    "    bag_of_words_y_pred_test = bag_of_words_pipeline.predict(X_test)\n",
    "\n",
    "    cm_count = confusion_matrix(y_train, y_pred_bag_of_words_cross_validation, normalize='true')\n",
    "    np.savetxt(\"SVM_BOW_unweighted_enhanced_cross_validation_confusion_matrix\" + str(n_gram_range) + '.txt', cm_count,\n",
    "               delimiter=',', fmt='%f')\n",
    "\n",
    "    result[\"SVM_BOW_unweighted_enhanced_predictions_testSet\" + str(n_gram_range)] = metrics.classification_report(y_test,\n",
    "                                                                                                                bag_of_words_y_pred_test)\n",
    "\n",
    "    filename = \"SVM_BOW_unweighted_enhanced_model.pickle\"\n",
    "    # save model\n",
    "    pickle.dump(bag_of_words_pipeline, open(filename, \"wb\"))\n",
    "\n",
    "    full_x = pd.concat([X_train, X_test])\n",
    "    full_y = pd.concat([y_train, y_test])\n",
    "\n",
    "    bag_of_words_grid_search.fit(full_x, full_y)\n",
    "    bag_of_words_pipeline.set_params(**bag_of_words_grid_search.best_params_)\n",
    "    bag_of_words_pipeline.fit(full_x, full_y)\n",
    "\n",
    "    filename = \"SVM_BOW_unweighted_enhanced_model_full.pickle\"\n",
    "    pickle.dump(bag_of_words_pipeline, open(filename, \"wb\"))\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n",
    "def save_dict_to_file(dictionary, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(dictionary, file)\n",
    "\n",
    "\n",
    "save_dict_to_file(result, 'SVM_unWeighted_enhanced.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9eea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
